{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48fac625-d376-4f28-a560-54312de6a655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw Data Sample ---\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "\n",
      "--- Missing Values Before Processing ---\n",
      "sepal length (cm)    0\n",
      "sepal width (cm)     0\n",
      "petal length (cm)    0\n",
      "petal width (cm)     0\n",
      "target               0\n",
      "dtype: int64\n",
      "\n",
      "--- Missing Values After Processing ---\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "dtype: int64\n",
      "\n",
      "--- Transformed Data Sample ---\n",
      "          0         1         2         3\n",
      "0  0.000000  1.019004 -1.340227 -1.315444\n",
      "1 -1.152203 -0.131979 -1.340227 -1.315444\n",
      "2 -1.395201  0.328414 -1.397064 -1.315444\n",
      "3 -1.516700  0.098217 -1.283389 -1.315444\n",
      "4 -1.030704  1.249201 -1.340227 -1.315444\n",
      "\n",
      " Preprocessed Data Saved to processed_iris_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Step 1: Extract - Load Data\n",
    "def extract_data():\n",
    "    iris = load_iris(as_frame=True)\n",
    "    df = iris.frame\n",
    "    \n",
    "    print(\"\\n--- Raw Data Sample ---\")\n",
    "    print(df.head())  # Show first 5 rows of raw data\n",
    "    \n",
    "    print(\"\\n--- Missing Values Before Processing ---\")\n",
    "    print(df.isnull().sum())  # Check missing values\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Step 2: Transform - Preprocessing Function\n",
    "def transform_data(df):\n",
    "    # Introduce missing values for demonstration\n",
    "    df.iloc[0, 0] = None  # Set a value in the first column to NaN\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    num_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    cat_features = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Pipelines for transformation\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"mean\")),  # Fill missing values\n",
    "        ('scaler', StandardScaler())                 # Scale features\n",
    "    ])\n",
    "\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),  # Fill missing categorical values\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))    # One-hot encode categorical data\n",
    "    ])\n",
    "\n",
    "    # Combine transformations\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ])\n",
    "\n",
    "    # Apply transformations\n",
    "    transformed_data = preprocessor.fit_transform(df)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    transformed_df = pd.DataFrame(transformed_data)\n",
    "    \n",
    "    print(\"\\n--- Missing Values After Processing ---\")\n",
    "    print(transformed_df.isnull().sum())  # Check if missing values are handled\n",
    "    \n",
    "    print(\"\\n--- Transformed Data Sample ---\")\n",
    "    print(transformed_df.head())  # Show first 5 rows of transformed data\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "# Step 3: Load - Save the cleaned data\n",
    "def load_data(df, output_path):\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n Preprocessed Data Saved to {output_path}\")\n",
    "\n",
    "# Run the ETL Pipeline\n",
    "if __name__ == \"__main__\":  \n",
    "    output_file = \"processed_iris_data.csv\"\n",
    "\n",
    "    # ETL Process\n",
    "    raw_df = extract_data()\n",
    "    cleaned_df = transform_data(raw_df)\n",
    "    load_data(cleaned_df, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef3e2d-4928-4b5c-b4bf-a12289c0c222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
